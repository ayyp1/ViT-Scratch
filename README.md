# ViT-Scratch
Just implementation of the official Vision transformer architecture according to [`ViT paper`](https://arxiv.org/pdf/2010.11929v2). Use **MNISt** dataset for classifying handwritten digits to test the architecture's working. 
